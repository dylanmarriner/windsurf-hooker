#!/usr/bin/env python3
"""
pre_write_diff_quality: Enforce diff hygiene, not correctness.

Checks:
- Diff too large? (warns, suggests breaking into smaller edits)
- Multiple concerns in one edit? (warns)
- Generated code without comments? (warns in SHIP mode)
- Diff coherence (single responsibility)

Invariant:
- Quality gates reduce entropy; they do not enforce correctness
- Warnings â†’ context annotations, never blocking (except in SHIP mode)
- No enforcement pre-write; quality is post-intent verification
"""

import json
import sys
import re
from typing import List, Dict

# Thresholds for warnings
LINES_PER_EDIT_WARNING = 1000  # Warn if single edit exceeds this
TOTAL_LINES_WARNING = 5000  # Warn if total changes exceed this
MAX_EDITS_IN_SINGLE_PASS = 10  # Warn if >N files edited at once
MIN_COMMENT_RATIO = 0.1  # Generated code should be >=10% comments


def count_lines(text: str) -> int:
    """Count non-empty lines."""
    return len([l for l in text.splitlines() if l.strip()])


def is_comment_line(line: str) -> bool:
    """Check if line is a comment."""
    s = line.strip()
    return s.startswith("#") or s.startswith("//") or s.startswith("--")


def is_generated_code(code: str) -> bool:
    """Detect if code looks auto-generated (lacks natural structure)."""
    lines = code.splitlines()
    if len(lines) < 5:
        return False

    # Patterns suggesting generated code
    generated_indicators = [
        r"^\s*(auto-generated|generated by|do not edit|autogenerated)",
        r"^\s*<\?xml|^\s*\{.*\}",  # XML/JSON-like
        r"^\s*\(\)\s*=>",  # Many arrow functions
        r"^\s*function\s+\w+\s*\(\s*\)\s*\{",  # Many function defs
    ]

    indicator_count = sum(
        1
        for line in lines
        for pattern in generated_indicators
        if re.search(pattern, line, re.IGNORECASE)
    )

    return indicator_count >= 3


def analyze_diffs(edits: List[Dict]) -> Dict[str, any]:
    """Analyze edit quality and coherence."""
    if not edits:
        return {"quality_pass": True, "warnings": []}

    warnings = []
    concerns = set()
    total_old_lines = 0
    total_new_lines = 0
    generated_code_blocks = 0
    files_edited = set()

    for edit in edits:
        old = edit.get("old_string", "") or ""
        new = edit.get("new_string", "") or ""
        path = edit.get("path", "unknown")

        files_edited.add(path)

        old_lines = count_lines(old)
        new_lines = count_lines(new)
        total_old_lines += old_lines
        total_new_lines += new_lines

        # Check edit size
        if max(old_lines, new_lines) > LINES_PER_EDIT_WARNING:
            warnings.append(
                f"Large single edit: {max(old_lines, new_lines)} lines in {path} "
                f"(consider breaking into smaller edits)"
            )

        # Detect generated code
        if is_generated_code(new):
            generated_code_blocks += 1

            # Check comment ratio
            comment_lines = sum(1 for l in new.splitlines() if is_comment_line(l))
            if new_lines > 0:
                ratio = comment_lines / new_lines
                if ratio < MIN_COMMENT_RATIO:
                    warnings.append(
                        f"Generated code in {path} lacks comments "
                        f"({int(ratio*100)}% comments, need >{int(MIN_COMMENT_RATIO*100)}%)"
                    )

        # Infer concerns from the changes
        if "test" in path.lower():
            concerns.add("testing")
        if "config" in path.lower() or path.endswith((".json", ".yaml", ".yml")):
            concerns.add("configuration")
        if any(lang in path for lang in [".py", ".ts", ".js"]):
            concerns.add("logic")
        if path.endswith((".md", ".txt")):
            concerns.add("documentation")

    # Check total edit size
    total_changes = total_old_lines + total_new_lines
    if total_changes > TOTAL_LINES_WARNING:
        warnings.append(
            f"Total changes are large ({total_changes} lines) - "
            f"consider breaking into smaller PRs"
        )

    # Check files edited count
    if len(files_edited) > MAX_EDITS_IN_SINGLE_PASS:
        warnings.append(
            f"Many files edited ({len(files_edited)}) - "
            f"consider breaking into logical groups"
        )

    # Check concern mixing (should generally be <=2 concerns per edit session)
    if len(concerns) > 2:
        warnings.append(
            f"Multiple concerns in single edit: {', '.join(sorted(concerns))} - "
            f"consider separating changes by concern"
        )

    return {
        "quality_pass": len(warnings) == 0,
        "warnings": warnings,
        "metrics": {
            "files_edited": len(files_edited),
            "total_old_lines": total_old_lines,
            "total_new_lines": total_new_lines,
            "generated_code_blocks": generated_code_blocks,
            "concerns": list(sorted(concerns)),
        },
    }


def main():
    try:
        payload = json.load(sys.stdin)
    except json.JSONDecodeError:
        print(json.dumps({"quality_pass": True, "warnings": []}))
        sys.exit(0)

    edits = (payload.get("tool_info", {}) or {}).get("edits", [])
    context = payload.get("conversation_context", "") or ""

    analysis = analyze_diffs(edits)

    # In SHIP mode, treat warnings as errors
    in_ship_mode = "[MODE:SHIP]" in context or "[MODE:SHIP_OK]" in context
    if in_ship_mode and not analysis["quality_pass"]:
        print("BLOCKED: quality gate failed in SHIP mode", file=sys.stderr)
        for warning in analysis["warnings"]:
            print(f"  - {warning}", file=sys.stderr)
        sys.exit(2)

    # Otherwise, emit analysis for context
    print(json.dumps(analysis))

    sys.exit(0)


if __name__ == "__main__":
    main()
